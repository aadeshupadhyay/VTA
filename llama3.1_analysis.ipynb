{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e2e487c-11e2-4f48-bff1-a81e691feb46",
   "metadata": {},
   "source": [
    "# We already have a dumped csv files based on intent for further analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336cb6ea-460d-437b-a30d-bdc1008a298d",
   "metadata": {},
   "source": [
    "## Analysis of Lecture Slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3025228a-11b5-4ed3-be2a-b8dc1352f745",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: \"3\\n\\nThe predicted paragraph closely aligns with the true paragraph, accurately reflecting key points and ideas. The content is highly relevant, covering main concepts such as protocols, internet str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m lecture_objectives\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwith lecture hint and lecture index\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwith lecture hint\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwithout lecture hint\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m llm_evaluation\n\u001b[0;32m----> 9\u001b[0m evaluation\u001b[38;5;241m=\u001b[39m\u001b[43mllm_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrue_answer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpred_answer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllama3.1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlecture\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/volunteer/AutoGTA/evaluation.py:30\u001b[0m, in \u001b[0;36mllm_evaluation\u001b[0;34m(true_paragraph, predicted_paragraph, model, mode)\u001b[0m\n\u001b[1;32m     28\u001b[0m     output\u001b[38;5;241m=\u001b[39mllm(prompt\u001b[38;5;241m+\u001b[39minputs)\n\u001b[1;32m     29\u001b[0m     llm_eval\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[0;32m---> 30\u001b[0m llm_eval\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m llm_eval]\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Plotting the transformed data\u001b[39;00m\n\u001b[1;32m     32\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: \"3\\n\\nThe predicted paragraph closely aligns with the true paragraph, accurately reflecting key points and ideas. The content is highly relevant, covering main concepts such as protocols, internet str"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "lecture_df=pd.read_csv(\"dump/lecture/output.csv\")\n",
    "true_answer=list(lecture_df.true_answer.values)\n",
    "pred_answer=list(lecture_df.pred_answer.values)\n",
    "true_obj=list(lecture_df.objectives.values)\n",
    "\n",
    "lecture_objectives=['with lecture hint and lecture index', 'with lecture hint','without lecture hint']\n",
    "from evaluation import llm_evaluation\n",
    "evaluation=llm_evaluation(true_answer,pred_answer,model=\"llama3.1\",mode=\"lecture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684ad35f-dfe2-476c-9c06-5d77cba239dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with evaluation and objectives\n",
    "data = {\n",
    "    'Objectives': lecture_objectives,\n",
    "    'Evaluation': evaluation\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(df['Objectives'], df['Evaluation'], color='skyblue')\n",
    "plt.xlabel('Objectives')\n",
    "plt.ylabel('LLM Evaluation Scores')\n",
    "plt.title('LLM Evaluation Results by Objectives')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(0, 4)  # Adjust this depending on your evaluation scale (0-4)\n",
    "plt.grid(axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420b0488-6ac2-4dcb-913e-2c3dcdd4dde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of homework slides\n",
    "homework_df=pd.read_csv(\"dump/homework/output.csv\")\n",
    "true_answer=list(lecture_df.true_answer.values)\n",
    "pred_answer=list(lecture_df.pred_answer.values)\n",
    "true_obj=list(lecture_df.objectives.values)\n",
    "\n",
    "lecture_objectives=['with lecture hint and lecture index', 'with lecture hint','without lecture hint']\n",
    "from evaluation import llm_evaluation\n",
    "evaluation=llm_hw_evaluation(true_answer,pred_answer,model=\"llama3.1\",mode=\"lecture\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
